{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fundamental-excuse",
   "metadata": {},
   "source": [
    "# Comparing different sampling techniques\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proof-literature",
   "metadata": {},
   "source": [
    "Run the next cell if you don't have numpy and nltk install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collectible-medium",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-vector",
   "metadata": {},
   "source": [
    "Run the next cell to download the Brown corpus. If you already have it you can skip it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "separated-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "sensitive-breathing",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import unicodedata\n",
    "from nltk.corpus import brown\n",
    "from BigramLM import BigramLM\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "parallel-patrol",
   "metadata": {},
   "source": [
    "## Training your LM\n",
    "\n",
    "Don't change the code in the next cell. Run it once to train the LM, it should take a few seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "removable-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_lm = BigramLM()\n",
    "bigram_lm.train(brown.sents()[:5000])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "protected-costa",
   "metadata": {},
   "source": [
    "bigram_lm.generate(w_i-1) -> P(w| w_i-1)  # Vector of dimension V indexed by word_id\n",
    "bigram_lm.dict[word] -> word_id \n",
    "bigram_lm.dict[word_id] -> word\n",
    "\n",
    "Also note: <BOS>= begining of sentence\n",
    "<EOS> = end of sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "australian-combination",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_sentence_with_top_1(lm):\n",
    "    sent = []\n",
    "    probs = []\n",
    "    counter = 0\n",
    "    curr_word = \"<BOS>\"\n",
    "    while curr_word != \"<EOS>\" and counter < 20:\n",
    "        next_word_id = np.argmax(lm.generate(curr_word))\n",
    "        next_word_prob = np.max(lm.generate(curr_word))\n",
    "        next_word = lm.dict[int(next_word_id)]\n",
    "        sent.append(next_word)\n",
    "        probs.append(next_word_prob)\n",
    "        curr_word = next_word\n",
    "        counter += 1\n",
    "    return sent, probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-compensation",
   "metadata": {},
   "source": [
    "You can run this to generate a sentence with corresponding word probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hundred-windows",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['The',\n",
       "  'President',\n",
       "  'Kennedy',\n",
       "  ',',\n",
       "  'and',\n",
       "  'the',\n",
       "  'first',\n",
       "  'time',\n",
       "  '.',\n",
       "  '<EOS>'],\n",
       " [0.04027680971821169,\n",
       "  0.0011894328283460623,\n",
       "  0.001383399209486166,\n",
       "  0.0007916611690196596,\n",
       "  0.019425019425019424,\n",
       "  0.008844475074661153,\n",
       "  0.0034525160802118806,\n",
       "  0.0006565126050420169,\n",
       "  0.0012504113195129978,\n",
       "  0.22478030731281154])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_sentence_with_top_1(bigram_lm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "allied-scottish",
   "metadata": {},
   "source": [
    "## Beam search (5 points)\n",
    "Similar to the above generate function, use beam search to generate the top 5 sentences. Use beam head = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enabling-campus",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your beam search code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "negative-sample",
   "metadata": {},
   "source": [
    "## Top-k sampling (5 points)\n",
    "Now write the same function to do top-k sampling and generate 5 sentences with k = 10, k = 100 each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "straight-marketplace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your top-k sampling code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "russian-slovakia",
   "metadata": {},
   "source": [
    "## Nucleus Sampling (5 points)\n",
    "Now rewrite the same function to perform nucleus sampling and generate 5 sentences with p = 0.95 and p = 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulated-pendant",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Nucleus sampling code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approved-settle",
   "metadata": {},
   "source": [
    "## Evaluation (5 points)\n",
    "Write a function that computes the perplexity of a sentence given a language model and a sampling technique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjustable-questionnaire",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dependent-tunnel",
   "metadata": {},
   "source": [
    "Compute the perplexity of the following sentences using nucleus sampling with p=0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-madrid",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = []\n",
    "test_sents[0] = ['Full','payment','of','nursing','home','bills','for','up','to','180','days','following','discharge','from','a','hospital','.']\n",
    "test_sents[1] = ['``','We',\"won't\",'know','the','full','amount','until','we','get','a','full','report',\"''\",',','Wagner','said','.']\n",
    "test_sents[2] = ['Each','ally','will','have','to','carry','out','obligations','long','since','laid','down',',','but','never','completely','fulfilled','.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "furnished-services",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Perplexity here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlikely-organ",
   "metadata": {},
   "source": [
    "Save your completed notebook with outputs as a pdf and submit it with the assignment. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "musical-hydrogen",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
